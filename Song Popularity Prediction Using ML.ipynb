{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3015803,"sourceType":"datasetVersion","datasetId":1847235}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-28T06:57:58.891650Z","iopub.execute_input":"2024-03-28T06:57:58.892821Z","iopub.status.idle":"2024-03-28T06:58:00.236608Z","shell.execute_reply.started":"2024-03-28T06:57:58.892765Z","shell.execute_reply":"2024-03-28T06:58:00.235429Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/song-popularity-dataset/song_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import math \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \nfrom IPython.display import display \n#from brokenaxes import brokenaxes from statsmodels.formula import api from sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.decomposition import PCA\nfrom sklearn.linear_model import Ridge from sklearn.linear_model import Lasso from sklearn.linear_model import ElasticNet from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error import matplotlib.pyplot as plt plt.rcParams['figure.figsize'] = [10,6] import warnings warnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T06:58:05.412750Z","iopub.execute_input":"2024-03-28T06:58:05.413370Z","iopub.status.idle":"2024-03-28T06:58:05.426793Z","shell.execute_reply.started":"2024-03-28T06:58:05.413325Z","shell.execute_reply":"2024-03-28T06:58:05.424905Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[2], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from statsmodels.stats.outliers_influence import variance_inflation_factor from sklearn.decomposition import PCA\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (4236097817.py, line 7)","output_type":"error"}]},{"cell_type":"code","source":"df = pd.read_csv('../input/song-popularity-dataset/song_data.csv') df.drop(['song_name'], axis=1, inplace=True) display(df.head()) target = 'song_popularity' features = [i for i in df.columns if i not in [target]] original_df = df.copy(deep=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0 rs,cs = original_df.shape df.drop_duplicates(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df.copy() ecc = nvc[nvc['Percentage']!=0].index.values fcc = [i for i in cf if i not in ecc] #One-Hot Binay Encoding oh=True dm=True for i in fcc: #print(i) if df3[i].nunique()==2: if oh==True: print(\"\\033[1mOne-Hot Encoding on features:\\033[0m\") print(i);oh=False df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i)) if (df3[i].nunique()>2 and df3[i].nunique()<17): if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df.copy() ecc = nvc[nvc['Percentage']!=0].index.values fcc = [i for i in cf if i not in ecc] #One-Hot Binay Encoding oh=True dm=True\nfor i in fcc: #print(i) if df3[i].nunique()==2: if oh==True: print(\"\\033[1mOne-Hot Encoding on features:\\033[0m\") print(i);oh=False df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i)) if (df3[i].nunique()>2 and df3[i].nunique()<17): if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\") print(i);dm=False df3 = pd.concat([df3.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df3[i], drop_first=True, prefix=str(i)))],ax","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df3.copy() #features1 = [i for i in features if i not in ['CHAS','RAD']] features1 = nf\nfor i in features1: Q1 = df1[i].quantile(0.25) Q3 = df1[i].quantile(0.75) IQR = Q3 - Q1 df1 = df1[df1[i] <= (Q3+(1.5*IQR))] df1 = df1[df1[i] >= (Q1-(1.5*IQR))] df1 = df1.reset_index(drop=True) display(df1.head())\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df1.copy() df.columns=[i.replace('-','_') for i in df.columns]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m=[] for i in df.columns.values: m.append(i.replace(' ','_')) df.columns = m X = df.drop([target],axis=1) Y = df[target] Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100) Train_X.reset_index(drop=True,inplace=True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std = StandardScaler() print('\\033[1mStandardardization on Training set'.center(120)) Train_X_std = std.fit_transform(Train_X) Train_X_std = pd.DataFrame(Train_X_std, columns=X.columns) display(Train_X_std.describe())\nprint('\\n','\\033[1mStandardardization on Testing set'.center(120)) Test_X_std = std.transform(Test_X) Test_X_std = pd.DataFrame(Test_X_std, columns=X.columns) display(Test_X_std.describe())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\033[1mCorrelation Matrix'.center(100)) plt.figure(figsize=[25,20]) sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, center=0) #cmap='BuGn' plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA pca = PCA().fit(Train_X_std) fig, ax = plt.subplots(figsize=(8,6)) x_values = range(1, pca.n_components_+1) ax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance') ax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red') plt.plot([0,pca.n_components_+1],[0.9,0.9],'g--')\nax.set_title('Explained variance of components') ax.set_xlabel('Principal Component') ax.set_ylabel('Explained Variance') plt.legend() plt.grid() plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA from sklearn.preprocessing import PolynomialFeatures Trr=[]; Tss=[]; n=3 order=['ord-'+str(i) for i in range(2,n)] Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order) Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order) m=df.shape[1]-1\nfor i in range(m): pca = PCA(n_components=Train_X_std.shape[1]-i) Train_X_std_pca = pca.fit_transform(Train_X_std) Test_X_std_pca = pca.fit_transform(Test_X_std) LR = LinearRegression() LR.fit(Train_X_std_pca, Train_Y) pred1 = LR.predict(Train_X_std_pca) pred2 = LR.predict(Test_X_std_pca) Trr.append(round(np.sqrt(mean_squared_error(Train_Y, pred1)),2)) Tss.append(round(np.sqrt(mean_squared_error(Test_Y, pred2)),2))\n    plt.plot(Trr, label='Train RMSE') plt.plot(Tss, label='Test RMSE') #plt.ylim([19.5,20.75]) plt.legend() plt.grid() plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}